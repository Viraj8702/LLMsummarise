{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import Pytesseract\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pdfs =  18\n",
      "PDF names:\n",
      "INQ000350057.pdf\n",
      "INQ000350094.pdf\n",
      "INQ000350513.pdf\n",
      "INQ000350691.pdf\n",
      "INQ000383581.pdf\n",
      "INQ000383585.pdf\n",
      "INQ000383998.pdf\n",
      "INQ000385719.pdf\n",
      "INQ000395589.pdf\n",
      "INQ000395913.pdf\n",
      "INQ000396684.pdf\n",
      "INQ000396685.pdf\n",
      "INQ000396686.pdf\n",
      "INQ000400585.pdf\n",
      "INQ000412042.pdf\n",
      "PHT000000014.pdf\n",
      "PHT000000030.pdf\n",
      "PHT000000039.pdf\n"
     ]
    }
   ],
   "source": [
    "# directory holding the sample of 18 pdfs\n",
    "samplepdf = '../1.Web Scrapping/PDF'\n",
    "\n",
    "# capturing all the pdfs\n",
    "# allpdf = os.listdir(samplepdf)\n",
    "pdffile = os.listdir(samplepdf)\n",
    "\n",
    "# displaying total number of pdfs used\n",
    "totalpdfs = len(pdffile)\n",
    "\n",
    "print(\"Total pdfs = \",totalpdfs)\n",
    "\n",
    "print(\"PDF names:\")\n",
    "allfiles=[]\n",
    "for pdf in pdffile:\n",
    "    allfiles.append(pdf)\n",
    "\n",
    "for names in allfiles:\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INQ000350057.pdf\n",
      "INQ000350094.pdf\n",
      "INQ000350513.pdf\n",
      "INQ000350691.pdf\n",
      "INQ000383581.pdf\n",
      "INQ000383585.pdf\n",
      "INQ000383998.pdf\n",
      "INQ000385719.pdf\n",
      "INQ000395589.pdf\n",
      "INQ000395913.pdf\n",
      "INQ000396684.pdf\n",
      "INQ000396685.pdf\n",
      "INQ000396686.pdf\n",
      "INQ000400585.pdf\n",
      "INQ000412042.pdf\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "considerfile=[filename for filename in allfiles if filename.startswith(\"INQ\")]\n",
    "\n",
    "\n",
    "# filtering to use only pdf which has INQ number in filename [Note: these are evidence docs]\n",
    "for filename in considerfile:\n",
    "    print(filename)\n",
    "\n",
    "\n",
    "considerPDFcount=len(considerfile)\n",
    "# number of pdf documents\n",
    "print(considerPDFcount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING PYPDF2 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INQ000350057.pdf\n",
      "INQ000350094.pdf\n",
      "INQ000350513.pdf\n",
      "INQ000350691.pdf\n",
      "INQ000383581.pdf\n",
      "INQ000383585.pdf\n",
      "INQ000383998.pdf\n",
      "INQ000385719.pdf\n",
      "INQ000395589.pdf\n",
      "INQ000395913.pdf\n",
      "INQ000396684.pdf\n",
      "INQ000396685.pdf\n",
      "INQ000396686.pdf\n",
      "INQ000400585.pdf\n",
      "INQ000412042.pdf\n"
     ]
    }
   ],
   "source": [
    "def pydf2extract(pdffiles):\n",
    "\n",
    "    with open(pdffiles,'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "    \n",
    "        results = []\n",
    "\n",
    "        for page in (reader.pages):\n",
    "            \n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                results.append(text)\n",
    "    return ' '.join(results) # convert list to a single doc \n",
    "\n",
    "textdocspypdf='PYPDF2/PYPDF2docs/'\n",
    "#if directory is not present, create it\n",
    "os.makedirs(textdocspypdf, exist_ok=True)\n",
    "\n",
    "for i in range(0,considerPDFcount):\n",
    "    try:\n",
    "        # pdfs with INQ\n",
    "        filename=considerfile[i]\n",
    "        print(filename)\n",
    "        \n",
    "        # extracting the text from the pdf\n",
    "        extractedtext = pydf2extract(f'../1.Web Scrapping/PDF/{filename}')\n",
    "        file = os.path.splitext(filename)[0]\n",
    "        # creating and storing the result into the directory\n",
    "        with open(os.path.join(textdocspypdf, f'{file}.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(extractedtext)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception for PDF {filename} as {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: INQ000350057.pdf\n",
      "Title: INQ000350057 - Paper from Welsh Government Technical Advisory Group titled statement regarding NPIs in the pre-christmas period, dated 02/12/2020.\n",
      "Header: Technical Advisory Group \n",
      "Date: 02 December 2020\n",
      "\n",
      "\n",
      "Filename: INQ000350094.pdf\n",
      "Title: INQ000350094 - Emails between Lee Waters (Deputy Minister for Economy and Transport) and (Private Secretary Deputy Minister Economy and Transport, Welsh Government) regarding a review of post-firebreak restrictions, between 19/11/2020 and 15/12/2020.\n",
      "Header: Message \n",
      "Date: 15 December 2020\n",
      "\n",
      "\n",
      "Filename: INQ000350513.pdf\n",
      "Title: INQ000350513 - Email chain between Chris Whitty (Chief Medical Officer for England, UK Government), Frank Atherton (Chief Medical Officer, Welsh Government), Rob Orford (Chief Scientific Adviser for Health, Welsh Government) and colleagues, regarding an overview of UK Strategy, dated 22/03/2020.\n",
      "Header: Message \n",
      "Date: 22 March 2020\n",
      "\n",
      "\n",
      "Filename: INQ000350691.pdf\n",
      "Title: INQ000350691 - Statement by Jane Hutt (Deputy First Minister) and Welsh Government, titled Statement to mark the International Day of Disabled People, dated 01/12/2020.\n",
      "Header: Llywodraeth Cymru \n",
      "Date: 1 December 2020\n",
      "\n",
      "\n",
      "Filename: INQ000383581.pdf\n",
      "Title: INQ000383581 - Minutes of Precautionary SAGE Meeting on Wuhan Coronavirus (WN-CoV) regarding situation update, current understanding of WN-CoV, NERVTAG conclusions, transport-related issues, UK health readiness and planning and HMG response, dated 23/01/2020.\n",
      "Header: OFFICIAL -SENSITIVE \n",
      "Date: 22 January 2020\n",
      "\n",
      "\n",
      "Filename: INQ000383585.pdf\n",
      "Title: INQ000383585 - Email from DHSC colleague to Sir Frank Atherton (Chief Medical Officer for Wales), Professor Chris Whitty (Chief Medical Officer for England), Catherine Calderwood (Chief Medical Officer for Northern Ireland) and Michael McBride (Chief Medical Officer for Nothern Ireland) regarding CMO Coronavirus briefing for DsPH, dated 05/02/2020.\n",
      "Header: Message \n",
      "Date: 31 Jan 2020\n",
      "\n",
      "\n",
      "Filename: INQ000383998.pdf\n",
      "Title: INQ000383998 - Email chain between Dr Rob Orford (Chief Scientific Adviser for Health, Welsh Government), PS Minister of Health and Social Services and colleagues, regarding Welsh National COVID 19 Test Plan, dated 27/03/2020.\n",
      "Header: Message \n",
      "Date: 27 March 2020\n",
      "\n",
      "\n",
      "Filename: INQ000385719.pdf\n",
      "Title: INQ000385719 - Email chain between Rob Orford (Chief Scientific Adviser for Health, Welsh Government), Frank Atherton (Chief Medical Officer for Wales), Tracey Cooper (Chief Executive of Public Health Wales) and colleagues, regarding PHW/WG - National assessment, dated between 10/11/2020 and 11/10/2020.\n",
      "Header: Message \n",
      "Date: 11 October 2020\n",
      "\n",
      "\n",
      "Filename: INQ000395589.pdf\n",
      "Title: INQ000395589 - Email chain between Gill Richardson (Professional Advisor to the Chief Medical Officer, Welsh Government) and Welsh Government and Public Health Wales colleagues, regarding a DHSC/PHE/DA call, dated between 14/04/2020 and 15/04/2020.\n",
      "Header: Message \n",
      "Date: 15 April 2020\n",
      "\n",
      "\n",
      "Filename: INQ000395913.pdf\n",
      "Title: INQ000395913 - Email chain between Dr Rob Orford (Chief Scientific Adviser for Health, Welsh Government), Reg Kilpatrick (Director General, Covid Crisis Coordination, Welsh Government) and colleagues, regarding Covid fire break, dated between 15/10/2020 and 18/10/2020.\n",
      "Header: Message \n",
      "Date: 18 October 2020\n",
      "\n",
      "\n",
      "Filename: INQ000396684.pdf\n",
      "Title: INQ000396684 - Guidance from the Cabinet Office / The National Archives titled Guidance on Private Office Records, dated 17/06/2009.\n",
      "Header: GUIDANCE ON PRIVATE OFFICE RECORDS \n",
      "Date: 17 June 2009\n",
      "\n",
      "\n",
      "Filename: INQ000396685.pdf\n",
      "Title: INQ000396685 - Paper from the Welsh Government titled Information Management and Governance Policy, dated February 2019\n",
      "Header: Llywodraeth Cymru \n",
      "Date: 1 April 2019\n",
      "\n",
      "\n",
      "Filename: INQ000396686.pdf\n",
      "Title: INQ000396686 - Notice from Senior Information Risk Officer, Welsh Government titled Social Media, dated 28/01/2020.\n",
      "Header: SIRO NOTICE • • • • • \n",
      "Date: 28 January 2020\n",
      "\n",
      "\n",
      "Filename: INQ000400585.pdf\n",
      "Title: INQ000400585 - Witness statement provided by Dr Christopher Williams on behalf of Public Health Wales,  dated 19/01/2024.\n",
      "Header: Witness Name: Dr Chris Williams \n",
      "Date: 19 January 2024\n",
      "\n",
      "\n",
      "Filename: INQ000412042.pdf\n",
      "Title: INQ000412042 - Presentation from UK Covid-19 Inquiry, titled module 2B data and charts, dated between 01/01/2020 and 01/04/2022.\n",
      "Header: INQ000412042_0001 \n",
      "Date: 1 Apr 2020\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# function to read and match with csv file\n",
    "def loadcsv(csvpath):\n",
    "    df = pd.read_csv(csvpath, delimiter=',') \n",
    "    filetitle = {}\n",
    "    for i, row in df.iterrows():\n",
    "        filename = row['Title'].split(' - ')[0]  # copy only the first name of the file and match with the full title, appering before '-'\n",
    "        filetitle[filename] = row['Title']  # match with the correspondong row of that title\n",
    "    return filetitle\n",
    "\n",
    "# extraction of date\n",
    "def extract_date(content):\n",
    "    datepattern = [\n",
    "        r'\\b\\d{1,2}\\s+\\w+\\s+\\d{4}\\b',      # dates like 21st December 2020\n",
    "        r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b',      # dates like 22/01/2020\n",
    "        r'\\b\\d{1}\\s+\\w+\\s+\\d{4}\\b'         # dates like 1 December 2020\n",
    "    ]\n",
    "    for pattern in datepattern:\n",
    "        match = re.search(pattern, content)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "    return \"No date found\"\n",
    "\n",
    "# obtaining title, header and date info\n",
    "def fileinfo(content):\n",
    "    lines = content.split('\\n')\n",
    "    header = lines[0] if lines else \"No header found\"  # Get the first line as the header\n",
    "    date = extract_date(content)\n",
    "    return header, date\n",
    "\n",
    "# file path of csv file which contains all the PDF's details from the website \n",
    "csvpath = '../1.Web Scrapping/AllFilesDetails.csv'\n",
    "filetitle = loadcsv(csvpath)\n",
    "\n",
    "# extracted content file path\n",
    "textdocscleaned = 'PYPDF2/PYPDF2docs/'\n",
    "results = []\n",
    "\n",
    "for i in range(considerPDFcount):\n",
    "    try:\n",
    "        filename = considerfile[i]\n",
    "        basefile = os.path.splitext(filename)[0]\n",
    "        txtpath = os.path.join(textdocscleaned, f'{basefile}.txt')\n",
    "        \n",
    "        if os.path.exists(txtpath):\n",
    "            with open(txtpath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # get title from csv which is store in filetitle\n",
    "            title = filetitle.get(basefile, \"No Title Found\")\n",
    "            header, date = fileinfo(content)\n",
    "\n",
    "            results.append({\n",
    "                \"Filename\": filename,\n",
    "                \"Title\": title,\n",
    "                \"Header\": header,\n",
    "                \"Date\": date\n",
    "            })\n",
    "            \n",
    "            print(f\"Filename: {filename}\\nTitle: {title}\\nHeader: {header}\\nDate: {date}\\n\\n\")\n",
    "        else:\n",
    "            # if path is not found append 'not exist'\n",
    "            print(f\"Text file {txtpath} does not exist.\")\n",
    "            results.append({\n",
    "                \"Filename\": filename,\n",
    "                \"Title\": \"Not exist\",\n",
    "                \"Header\": \"Not exist\",\n",
    "                \"Date\": \"Not exist\"\n",
    "            })\n",
    "    # handling exception \n",
    "    except Exception as e:\n",
    "        print(f\"Exception for {filename}: {e}\")\n",
    "        results.append({\n",
    "            \"Filename\": filename,\n",
    "            \"Title\": \"Exception\",\n",
    "            \"Header\": \"Exception\",\n",
    "            \"Date\": str(e)\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe and store the content\n",
    "df = pd.DataFrame(results)\n",
    "df\n",
    "df.to_csv('SamplePdfDetail.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Title</th>\n",
       "      <th>Header</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INQ000350057.pdf</td>\n",
       "      <td>INQ000350057 - Paper from Welsh Government Tec...</td>\n",
       "      <td>Technical Advisory Group</td>\n",
       "      <td>02 December 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INQ000350094.pdf</td>\n",
       "      <td>INQ000350094 - Emails between Lee Waters (Depu...</td>\n",
       "      <td>Message</td>\n",
       "      <td>15 December 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INQ000350513.pdf</td>\n",
       "      <td>INQ000350513 - Email chain between Chris Whitt...</td>\n",
       "      <td>Message</td>\n",
       "      <td>22 March 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INQ000350691.pdf</td>\n",
       "      <td>INQ000350691 - Statement by Jane Hutt (Deputy ...</td>\n",
       "      <td>Llywodraeth Cymru</td>\n",
       "      <td>1 December 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INQ000383581.pdf</td>\n",
       "      <td>INQ000383581 - Minutes of Precautionary SAGE M...</td>\n",
       "      <td>OFFICIAL -SENSITIVE</td>\n",
       "      <td>22 January 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INQ000383585.pdf</td>\n",
       "      <td>INQ000383585 - Email from DHSC colleague to Si...</td>\n",
       "      <td>Message</td>\n",
       "      <td>31 Jan 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INQ000383998.pdf</td>\n",
       "      <td>INQ000383998 - Email chain between Dr Rob Orfo...</td>\n",
       "      <td>Message</td>\n",
       "      <td>27 March 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INQ000385719.pdf</td>\n",
       "      <td>INQ000385719 - Email chain between Rob Orford ...</td>\n",
       "      <td>Message</td>\n",
       "      <td>11 October 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INQ000395589.pdf</td>\n",
       "      <td>INQ000395589 - Email chain between Gill Richar...</td>\n",
       "      <td>Message</td>\n",
       "      <td>15 April 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INQ000395913.pdf</td>\n",
       "      <td>INQ000395913 - Email chain between Dr Rob Orfo...</td>\n",
       "      <td>Message</td>\n",
       "      <td>18 October 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>INQ000396684.pdf</td>\n",
       "      <td>INQ000396684 - Guidance from the Cabinet Offic...</td>\n",
       "      <td>GUIDANCE ON PRIVATE OFFICE RECORDS</td>\n",
       "      <td>17 June 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>INQ000396685.pdf</td>\n",
       "      <td>INQ000396685 - Paper from the Welsh Government...</td>\n",
       "      <td>Llywodraeth Cymru</td>\n",
       "      <td>1 April 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>INQ000396686.pdf</td>\n",
       "      <td>INQ000396686 - Notice from Senior Information ...</td>\n",
       "      <td>SIRO NOTICE • • • • •</td>\n",
       "      <td>28 January 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>INQ000400585.pdf</td>\n",
       "      <td>INQ000400585 - Witness statement provided by D...</td>\n",
       "      <td>Witness Name: Dr Chris Williams</td>\n",
       "      <td>19 January 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>INQ000412042.pdf</td>\n",
       "      <td>INQ000412042 - Presentation from UK Covid-19 I...</td>\n",
       "      <td>INQ000412042_0001</td>\n",
       "      <td>1 Apr 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Filename                                              Title  \\\n",
       "0   INQ000350057.pdf  INQ000350057 - Paper from Welsh Government Tec...   \n",
       "1   INQ000350094.pdf  INQ000350094 - Emails between Lee Waters (Depu...   \n",
       "2   INQ000350513.pdf  INQ000350513 - Email chain between Chris Whitt...   \n",
       "3   INQ000350691.pdf  INQ000350691 - Statement by Jane Hutt (Deputy ...   \n",
       "4   INQ000383581.pdf  INQ000383581 - Minutes of Precautionary SAGE M...   \n",
       "5   INQ000383585.pdf  INQ000383585 - Email from DHSC colleague to Si...   \n",
       "6   INQ000383998.pdf  INQ000383998 - Email chain between Dr Rob Orfo...   \n",
       "7   INQ000385719.pdf  INQ000385719 - Email chain between Rob Orford ...   \n",
       "8   INQ000395589.pdf  INQ000395589 - Email chain between Gill Richar...   \n",
       "9   INQ000395913.pdf  INQ000395913 - Email chain between Dr Rob Orfo...   \n",
       "10  INQ000396684.pdf  INQ000396684 - Guidance from the Cabinet Offic...   \n",
       "11  INQ000396685.pdf  INQ000396685 - Paper from the Welsh Government...   \n",
       "12  INQ000396686.pdf  INQ000396686 - Notice from Senior Information ...   \n",
       "13  INQ000400585.pdf  INQ000400585 - Witness statement provided by D...   \n",
       "14  INQ000412042.pdf  INQ000412042 - Presentation from UK Covid-19 I...   \n",
       "\n",
       "                                 Header              Date  \n",
       "0             Technical Advisory Group   02 December 2020  \n",
       "1                              Message   15 December 2020  \n",
       "2                              Message      22 March 2020  \n",
       "3                    Llywodraeth Cymru    1 December 2020  \n",
       "4                  OFFICIAL -SENSITIVE    22 January 2020  \n",
       "5                              Message        31 Jan 2020  \n",
       "6                              Message      27 March 2020  \n",
       "7                              Message    11 October 2020  \n",
       "8                              Message      15 April 2020  \n",
       "9                              Message    18 October 2020  \n",
       "10  GUIDANCE ON PRIVATE OFFICE RECORDS       17 June 2009  \n",
       "11                   Llywodraeth Cymru       1 April 2019  \n",
       "12               SIRO NOTICE • • • • •    28 January 2020  \n",
       "13     Witness Name: Dr Chris Williams    19 January 2024  \n",
       "14                   INQ000412042_0001         1 Apr 2020  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('SamplePdfDetail.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Storing Non email documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INQ000350057.pdf\n",
      "INQ000350691.pdf\n",
      "INQ000383581.pdf\n",
      "INQ000396684.pdf\n",
      "INQ000396685.pdf\n",
      "INQ000396686.pdf\n",
      "INQ000400585.pdf\n",
      "INQ000412042.pdf\n"
     ]
    }
   ],
   "source": [
    "def pydf2extract(pdffiles):\n",
    "\n",
    "    with open(pdffiles,'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "    \n",
    "        results = []\n",
    "\n",
    "        for page in (reader.pages):\n",
    "            \n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                results.append(text)\n",
    "    return ' '.join(results) \n",
    "\n",
    "\n",
    "# remove whitespaces from the columns\n",
    "df.columns = df.columns.str.strip()  \n",
    "df['Header'] = df['Header'].str.strip()  \n",
    "\n",
    "# filter rows where header is 'Message'\n",
    "filterdf = df[df['Header'] != 'Message']\n",
    "\n",
    "\n",
    "textdocspypdf2 = 'PYPDF2/OtherExtraction/'\n",
    "os.makedirs(textdocspypdf2, exist_ok=True)\n",
    "\n",
    "for filename in filterdf['Filename']:\n",
    "    try:\n",
    "        print(f\"{filename}\")\n",
    "        pdfpath = f'../1.Web Scrapping/PDF/{filename}'\n",
    "\n",
    "        # extract content from pdf\n",
    "        extractedtext = pydf2extract(pdfpath)\n",
    "\n",
    "        file = os.path.splitext(filename)[0]\n",
    "        # storing the extracted text into the directory\n",
    "        outputpath = os.path.join(textdocspypdf2, f'{file}.txt')\n",
    "        with open(outputpath, 'w', encoding='utf-8') as f:\n",
    "            f.write(extractedtext)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception for PDF {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Storing Email documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INQ000350094.txt\n",
      "INQ000350513.txt\n",
      "INQ000383585.txt\n",
      "INQ000383998.txt\n",
      "INQ000385719.txt\n",
      "INQ000395589.txt\n",
      "INQ000395913.txt\n"
     ]
    }
   ],
   "source": [
    "def emailextract(filepath):\n",
    "    # reading the file\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # splitting multiple email into individual\n",
    "    emails = re.split(r'(?=From: )', text)\n",
    "\n",
    "    emaildetails = []\n",
    "    for email in emails:\n",
    "        if email:\n",
    "            emaildetails.append(email)\n",
    "        \n",
    "    # all the emails splitted are stored in their respective file\n",
    "    return emaildetails\n",
    "\n",
    "def directorydata(inputpath, outputpath):\n",
    "    if not os.path.exists(outputpath):\n",
    "        os.makedirs(outputpath)\n",
    "\n",
    "    for filename in os.listdir(inputpath):\n",
    "        if filename.endswith('.txt'):\n",
    "            filepath = os.path.join(inputpath, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                if 'Message' in content:\n",
    "                    emails = emailextract(filepath)\n",
    "                    outputfile = os.path.join(outputpath, filename)\n",
    "                    with open(outputfile, 'w', encoding='utf-8') as text:\n",
    "                        for i, email in enumerate(emails):\n",
    "                            text.write(f\"{email}\\n-------------------------\\n\\n\")\n",
    "                    print(f\"{filename}\")\n",
    "\n",
    "# file path which contains txt files\n",
    "inputpath = 'PYPDF2/PYPDF2docs/'\n",
    "# output directory to store the content\n",
    "outputpath = 'PYPDF2/ExtractedEmails/'\n",
    "directorydata(inputpath, outputpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding Paragraphs and storing in the directory__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For file INQ000350057.txt, 1 paragraphs are found\n",
      "For file INQ000350691.txt, 1 paragraphs are found\n",
      "For file INQ000383581.txt, 1 paragraphs are found\n",
      "For file INQ000396684.txt, 1 paragraphs are found\n",
      "For file INQ000396685.txt, 1 paragraphs are found\n",
      "For file INQ000396686.txt, 1 paragraphs are found\n",
      "For file INQ000400585.txt, 1 paragraphs are found\n",
      "For file INQ000412042.txt, 1 paragraphs are found\n",
      "For file INQ000350094.txt, 9 paragraphs are found\n",
      "For file INQ000350513.txt, 3 paragraphs are found\n",
      "For file INQ000383585.txt, 1 paragraphs are found\n",
      "For file INQ000383998.txt, 2 paragraphs are found\n",
      "For file INQ000385719.txt, 3 paragraphs are found\n",
      "For file INQ000395589.txt, 4 paragraphs are found\n",
      "For file INQ000395913.txt, 10 paragraphs are found\n"
     ]
    }
   ],
   "source": [
    "def findparagraphs(text):\n",
    "    # capture paragraphs with more than 2 lines\n",
    "    minlines = 2\n",
    "    \n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    multiparas = [p for p in paragraphs if len(p.split('\\n')) > minlines]\n",
    "    return multiparas\n",
    "\n",
    "# both directories to find the paragraphs, OtherExtraction and ExtractedEmails\n",
    "def storeparagraphs(directories, paradir):\n",
    "    \n",
    "    os.makedirs(paradir, exist_ok=True)\n",
    "    \n",
    "    for directory in directories:\n",
    "        # passing each files from the directory \n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                # finding the specific file from the specific directory\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    txtcontent = file.read()\n",
    "                # finding and counting the paragraphs\n",
    "                paragraphs = findparagraphs(txtcontent)\n",
    "                # storing the paragraphs into separate directory\n",
    "                outputfile = os.path.join(paradir, filename)\n",
    "                with open(outputfile, 'w', encoding='utf-8') as text:\n",
    "                    for para in paragraphs:\n",
    "                        text.write(para + '\\n\\n')\n",
    "                print(f\"For file {filename}, {len(paragraphs)} paragraphs are found\")\n",
    "\n",
    "# using both directories\n",
    "otherextractpath = 'PYPDF2/OtherExtraction/'     # contains all the documents except emails\n",
    "emailextractpath = 'PYPDF2/ExtractedEmails/'  # contains the documents which are only emails\n",
    "paradir = 'PYPDF2/CombinedParagraphs/'\n",
    "\n",
    "# passing the text files from both directories\n",
    "storeparagraphs([otherextractpath, emailextractpath], paradir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text Cleaning for non emails documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length for INQ000350057.pdf : 29967\n",
      "Cleaned text length for INQ000350057.pdf : 25996\n",
      "\n",
      "\n",
      "Original text length for INQ000350691.pdf : 7117\n",
      "Cleaned text length for INQ000350691.pdf : 6903\n",
      "\n",
      "\n",
      "Original text length for INQ000383581.pdf : 6607\n",
      "Cleaned text length for INQ000383581.pdf : 6148\n",
      "\n",
      "\n",
      "Original text length for INQ000396684.pdf : 26897\n",
      "Cleaned text length for INQ000396684.pdf : 25926\n",
      "\n",
      "\n",
      "Original text length for INQ000396685.pdf : 75251\n",
      "Cleaned text length for INQ000396685.pdf : 73155\n",
      "\n",
      "\n",
      "Original text length for INQ000396686.pdf : 11803\n",
      "Cleaned text length for INQ000396686.pdf : 11025\n",
      "\n",
      "\n",
      "Original text length for INQ000400585.pdf : 34274\n",
      "Cleaned text length for INQ000400585.pdf : 33189\n",
      "\n",
      "\n",
      "Original text length for INQ000412042.pdf : 5828\n",
      "Cleaned text length for INQ000412042.pdf : 4722\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('SamplePdfDetail.csv')\n",
    "\n",
    "# removing white spaces in the 'Header' columns\n",
    "df['Header'] = df['Header'].str.strip()\n",
    "\n",
    "# finding only the files which doesn't has in header column, as value 'Message'\n",
    "filterdf = df[df['Header'] != 'Message']\n",
    "\n",
    "# input directory which has only paragraphs\n",
    "paragraphdoc = 'PYPDF2/CombinedParagraphs'\n",
    "# ouput direcotry which will contain cleaned text\n",
    "cleanedtextdoc = 'PYPDF2/PYPDF2textclean'\n",
    "\n",
    "# output directory\n",
    "os.makedirs(cleanedtextdoc, exist_ok=True)\n",
    "\n",
    "def cleantxt(text):\n",
    "    # regex patterns for removing unwanted elements\n",
    "    patterns = [\n",
    "        r'OFFICIAL SENSITIVE',\n",
    "        r'OFFICIAL\\s*-\\s*SENSITIVE',                          \n",
    "        r'Llywodraeth Cymru',\n",
    "\n",
    "\n",
    "        # for handling INQ\n",
    "        r'I\\s*NQ000\\w*',                                  # INQ number starting with I followed by NQ000 and numbers/characters\n",
    "        r'I\\s*NQ000\\S+',   \n",
    "        r'I?\\s*NQ000\\d+_\\d+',                             # INQ number starting with \"INQ000\" and numbers/characters\n",
    "        r'INQ\\s*\\d{6,7}_\\d{4}',                           # specific footer code with optional spaces\n",
    "        r'I\\s*NQ\\s*\\d{6,7}_\\d{4}',                        # specific footer code with spaces between I, NQ, and numbers\n",
    "        r'\\bINQ\\s*\\d{6,7}',                               # INQ number without trailing part\n",
    "        r'I\\s*NQ\\d{6,7}_\\w+',                             # INQ code followed by alphanumeric\n",
    "        r'I\\s*NQ\\d{6,7}_\\d{4}',                           # I NQ followed by 6 or 7 digits, underscore, and 4 digits\n",
    "        r'I\\s*NQ\\d{6,7}_\\d+',                             # I NQ followed by 6 or 7 digits, underscore, and any number of digits\n",
    "        r'I\\s*NQ000\\d+_\\d+',                              # I NQ000 followed by digits, underscore, and any number of digits\n",
    "        \n",
    "        # for handling page numbers\n",
    "        r'Page\\s+\\d+\\s*$',                                # e.g. Page 1, Page 23\n",
    "        \n",
    "\n",
    "        # handling emails or links\n",
    "        r'\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b',     # email addresses\n",
    "        r'\\b(e)\\b\\s+',                                    # single 'e' with whitespace after\n",
    "        r'https?://\\S+',                                  # URLs starting with http or https\n",
    "        \n",
    "        # characters that captured unusual characters\n",
    "        r'\\.{3,}'                                           # removal of ....\n",
    "        r'|\\s?\\.\\s\\.\\s\\.\\s\\.',                               # removal of dots which contains whitespaces\n",
    "        r'|(_{3,}\\s?|_\\s?_+)',                               # removal of ____\n",
    "        r'|(-{3,}\\s?|-\\s?-+)',                               # removal of - - - - \n",
    "        r'|■|',                                              # removal of any unusual character\n",
    "        r'|(\\._){1,}',                                       # removal of ._ \n",
    "        r'\\b\\w*(\\w)\\1{3,}\\w*\\b'                             # removal of any character that is repeated more than 3 times\n",
    "        r'\\s+(\\d)\\1{6,}\\s+'                                   # removal of any same digits being repeated more than 6 times \n",
    "\n",
    "        # patterns to capture unconventional urls\n",
    "        r'https:[^\\s]+',                                  # captured URLs starting with https:\n",
    "        r'http:[^\\s]+',                                   # captured URLs starting with http:\n",
    "        r'\\bhttps?:\\S+\\s*',                               # capturing URLs which may be differently structured\n",
    "        r'\\bhttps?:\\S+(?:\\s*\\S)*',                        # urls which may have different pattern\n",
    "        r'\\bhttps?:\\/\\/[a-zA-Z0-9.-]+(\\/\\S*)?',           # urls with proper structure\n",
    "        r'\\b(?:http|https|ftp):\\/\\/[a-zA-Z0-9.-]+(?:\\/[^\\s]*)?', # General URLs\n",
    "        r'\\bhttps?:\\/\\/\\S+(?:\\s+\\S+)*',                   # urls split by spaces\n",
    "        r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\s+)+', # Emails split by spaces\n",
    "        r'\\bhttps?:\\/\\/[^\\s]+',                           # general capture for http/https urls\n",
    "        r'www\\.[^\\s]+',                                   # urls starting with www\n",
    "        r'ftp:\\/\\/[^\\s]+',                                # urls starting with ftp\n",
    "\n",
    "        \n",
    "    ]\n",
    "\n",
    "    # applying pattern to the each textual data\n",
    "    for pattern in patterns:\n",
    "        # if the pattern for INQ number matches with any of the following, then remove it\n",
    "        if pattern in [r'INQ\\s*\\d{6,7}_\\d{4}', r'I\\s*NQ\\s*\\d{6,7}_\\d{4}', r'\\bINQ\\s*\\d{6,7}', r'I\\s*NQ\\d{6,7}_\\w+']:\n",
    "            text = re.sub(r'\\s*' + pattern + r'\\s*', '', text)\n",
    "        # during extraction process, few documents consider bullet point as 'e'\n",
    "        elif pattern == r'\\b(e)\\b\\s+':\n",
    "            # convert single 'e' character back to bullet point\n",
    "            text = re.sub(pattern, '• ', text)\n",
    "        elif pattern == r'https?:(?:\\s*\\S)*':   \n",
    "            # remove that starts with https or http\n",
    "            text = re.sub(r'https?://\\S+', '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        else:\n",
    "            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            \n",
    "    # remove any extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# iterate through the filtered DataFrame\n",
    "for i, row in filterdf.iterrows():\n",
    "    filename = row['Filename']\n",
    "    f1name = os.path.splitext(filename)[0]\n",
    "    txtfilepath = os.path.join(paragraphdoc, f'{f1name}.txt')\n",
    "\n",
    "    # check if the text file exists\n",
    "    if os.path.exists(txtfilepath):\n",
    "\n",
    "        # read the content of the text file\n",
    "        with open(txtfilepath, 'r', encoding='utf-8') as f:\n",
    "            text_content = f.read()\n",
    "\n",
    "        print(f\"Original text length for {filename} : {len(text_content)}\")  \n",
    "\n",
    "        # clean the text content\n",
    "        cleanedtext = cleantxt(text_content)\n",
    "\n",
    "        print(f\"Cleaned text length for {filename} : {len(cleanedtext)}\\n\\n\")  \n",
    "\n",
    "        # write the cleaned text to a new file in the cleanedtextdocs directory\n",
    "        cleanedtxtpath = os.path.join(cleanedtextdoc, f'{f1name}.txt')\n",
    "        with open(cleanedtxtpath, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleanedtext)\n",
    "\n",
    "    else:\n",
    "        print(f\"{txtfilepath} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text Cleaning for Email Extraction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYPDF2/CombinedParagraphs\\INQ000350094.txt\n",
      "Email 1 (Word Count: 39):\n",
      "\n",
      "\n",
      "Email 2 (Word Count: 158):\n",
      "\n",
      "\n",
      "Email 3 (Word Count: 44):\n",
      "\n",
      "\n",
      "Email 4 (Word Count: 13):\n",
      "\n",
      "\n",
      "Email 5 (Word Count: 66):\n",
      "\n",
      "\n",
      "Email 6 (Word Count: 79):\n",
      "\n",
      "\n",
      "Email 7 (Word Count: 573):\n",
      "\n",
      "\n",
      "Email 8 (Word Count: 561):\n",
      "\n",
      "\n",
      "Email 9 (Word Count: 280):\n",
      "\n",
      "\n",
      "PYPDF2/CombinedParagraphs\\INQ000350513.txt\n",
      "Email 1 (Word Count: 349):\n",
      "\n",
      "\n",
      "Email 2 (Word Count: 15):\n",
      "\n",
      "\n",
      "Email 3 (Word Count: 143):\n",
      "\n",
      "\n",
      "PYPDF2/CombinedParagraphs\\INQ000383585.txt\n",
      "Email 1 (Word Count: 426):\n",
      "\n",
      "\n",
      "PYPDF2/CombinedParagraphs\\INQ000383998.txt\n",
      "Email 1 (Word Count: 34):\n",
      "\n",
      "\n",
      "Email 2 (Word Count: 586):\n",
      "\n",
      "\n",
      "PYPDF2/CombinedParagraphs\\INQ000385719.txt\n",
      "Email 1 (Word Count: 0):\n",
      "\n",
      "\n",
      "Email 2 (Word Count: 439):\n",
      "\n",
      "\n",
      "Email 3 (Word Count: 69):\n",
      "\n",
      "\n",
      "PYPDF2/CombinedParagraphs\\INQ000395589.txt\n",
      "Email 1 (Word Count: 203):\n",
      "\n",
      "\n",
      "Email 2 (Word Count: 114):\n",
      "\n",
      "\n",
      "Email 3 (Word Count: 532):\n",
      "\n",
      "\n",
      "Email 4 (Word Count: 1124):\n",
      "\n",
      "\n",
      "PYPDF2/CombinedParagraphs\\INQ000395913.txt\n",
      "Email 1 (Word Count: 282):\n",
      "\n",
      "\n",
      "Email 2 (Word Count: 50):\n",
      "\n",
      "\n",
      "Email 3 (Word Count: 181):\n",
      "\n",
      "\n",
      "Email 4 (Word Count: 132):\n",
      "\n",
      "\n",
      "Email 5 (Word Count: 155):\n",
      "\n",
      "\n",
      "Email 6 (Word Count: 350):\n",
      "\n",
      "\n",
      "Email 7 (Word Count: 229):\n",
      "\n",
      "\n",
      "Email 8 (Word Count: 86):\n",
      "\n",
      "\n",
      "Email 9 (Word Count: 535):\n",
      "\n",
      "\n",
      "Email 10 (Word Count: 22):\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('SamplePdfDetail.csv')\n",
    "\n",
    "# removing white spaces in the 'Header' columns\n",
    "df['Header'] = df['Header'].str.strip()\n",
    "\n",
    "# finding only the files which have 'Message' in the 'Header' column\n",
    "filterdf = df[df['Header'] == 'Message']\n",
    "\n",
    "# combined paragraphs contains only the paragraphs captured \n",
    "emailtxt = 'PYPDF2/CombinedParagraphs'\n",
    "# output directory to store the cleaned text \n",
    "cleanedtextdoc = 'PYPDF2/PYPDF2textclean/'\n",
    "\n",
    "# ensure the output directory exists\n",
    "os.makedirs(cleanedtextdoc, exist_ok=True)\n",
    "\n",
    "def emailclean(txt_path):\n",
    "    # read text file\n",
    "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # split the text into individual emails based on the 'From:' keyword\n",
    "    emails = re.split(r'(?=From: )', text)\n",
    "\n",
    "    emailinfo = []\n",
    "    for email in emails:\n",
    "\n",
    "        emailheader = (\n",
    "            r'From:.*?\\n'  \n",
    "            r'Sent:.*?\\n'  \n",
    "            # r'To:.*?\\n'    # removing this affects some files to lose information\n",
    "            r'(Cc:.*?\\n)?'  \n",
    "            r'Subject:.*?\\n'  \n",
    "        )\n",
    "        emailcontent = re.sub(emailheader, '', email, flags=re.DOTALL)\n",
    "        \n",
    "        # remove any unncessary content from the emails\n",
    "        emailcontent = re.sub(r'\\S+@\\S+', '', emailcontent)  # remove email addresses\n",
    "        emailcontent = re.sub(r'<[^>]+>', '', emailcontent)  # remove anything in angle brackets\n",
    "        emailcontent = re.sub(r'(\\w+\\.\\w+@\\w+\\.\\w+)', '', emailcontent)  # remove name and email in the format: name@domain\n",
    "        \n",
    "        # for handling INQ\n",
    "        emailcontent = re.sub(r'I\\s*NQ000\\w*','',emailcontent)                    # INQ number starting with 'I' followed by NQ000 and numbers/characters\n",
    "        emailcontent = re.sub(r'I\\s*NQ000\\S+','',emailcontent)   \n",
    "        emailcontent = re.sub(r'I?\\s*NQ000\\d+_\\d+','',emailcontent)               # INQ number starting with INQ000 and numbers/characters\n",
    "        emailcontent = re.sub(r'https?://\\S+','',emailcontent)                    # urls starting with http or https\n",
    "        emailcontent = re.sub(r'INQ\\s*\\d{6,7}_\\d{4}','',emailcontent)             # specific footer code with optional spaces\n",
    "        emailcontent = re.sub(r'I\\s*NQ\\s*\\d{6,7}_\\d{4}','',emailcontent)          # specific footer code with spaces between I NQ and numbers\n",
    "        emailcontent = re.sub(r'\\bINQ\\s*\\d{6,7}','',emailcontent)                 # INQ number without trailing part\n",
    "        emailcontent = re.sub(r'I\\s*NQ\\d{6,7}_\\w+','',emailcontent)               # INQ code followed by alphanumeric characters\n",
    "       \n",
    "\n",
    "        # remove sequences like ._. ._. ._. ._. ._. ._. ._. ._. ._. ._. ._. \n",
    "        emailcontent = re.sub(r'\\._\\.(\\s*\\._\\.)*', '', emailcontent)\n",
    "        \n",
    "        # remove sequences like .-. .-. .-. .-. .-. .-. .-. .-. .-. .-. .-. \n",
    "        emailcontent = re.sub(r'\\.-\\.(\\s*\\.-\\.)*', '', emailcontent)\n",
    "\n",
    "        # remove 'NR' (assuming case insensitive)\n",
    "        emailcontent = re.sub(r'\\bNR\\b', '', emailcontent, flags=re.IGNORECASE)\n",
    "        \n",
    "        # remove 'Name Redacted' (assuming case insensitive)\n",
    "        emailcontent = re.sub(r'\\bName\\s+Redacted\\b', '', emailcontent, flags=re.IGNORECASE)\n",
    "        emailcontent = re.sub(r'\\bNameRedacted\\b', '', emailcontent, flags=re.IGNORECASE)\n",
    "         \n",
    "        # remove \"Ebost/Email\"\n",
    "        emailcontent = re.sub(r'Ebost/Email', '', emailcontent)\n",
    "        emailcontent = re.sub(r'E-bost', '', emailcontent)\n",
    "        emailcontent = re.sub(r'E-mail', '', emailcontent)\n",
    "        emailcontent = re.sub(r'/E-mail', '', emailcontent)\n",
    "        emailcontent = re.sub(r'/Email', '', emailcontent)\n",
    "        emailcontent = re.sub(r'E-bost/E-mail', '', emailcontent)\n",
    "\n",
    "        # remove sequences like ____________________\n",
    "        emailcontent = re.sub(r'_+', '', emailcontent)\n",
    "\n",
    "        if emailcontent:\n",
    "            emailinfo.append(emailcontent)\n",
    "    return emailinfo\n",
    "\n",
    "def countwords(text):\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    return len(words)\n",
    "\n",
    "# using filetered_df which has the documents that contains only the emails\n",
    "for i, row in filterdf.iterrows():\n",
    "    filename = row['Filename']\n",
    "    f1name = os.path.splitext(filename)[0]\n",
    "    txtfilepath = os.path.join(emailtxt, f'{f1name}.txt')\n",
    "\n",
    "    \n",
    "    if os.path.exists(txtfilepath):\n",
    "        print(f\"{txtfilepath}\")\n",
    "\n",
    "        # cleaning the emails by removing any unncessary data\n",
    "        emails = emailclean(txtfilepath)\n",
    "\n",
    "        # prepare the content to be written to the output file\n",
    "        combinedemail = \"\"\n",
    "        for i, email in enumerate(emails):\n",
    "            # Count words in the email content\n",
    "            wordcount = countwords(email)\n",
    "            print(f\"Email {i+1} (Word Count: {wordcount}):\\n\\n\")\n",
    "            \n",
    "            combinedemail += f\"{email}\"\n",
    "\n",
    "        # write all the emails content to a single file in the output directory\n",
    "        outputfile = os.path.join(cleanedtextdoc, f'{f1name}.txt')\n",
    "        with open(outputfile, 'w', encoding='utf-8') as f:\n",
    "            f.write(combinedemail)\n",
    "    else:\n",
    "        print(f\"Text file {txtfilepath} does not exist.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
